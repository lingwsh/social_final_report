{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: A prosperous but noisy city: Noise analysis in New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used for the project is the records of 311 Service Requests, a government hotline from the NYC open data website which reflects the daily life problems of many residents. [Link](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) <br>The dataset includes government hotlines' records from 2010 to the present, about a decade, covering all aspects of residents' daily life.\n",
    "\n",
    "<br>\n",
    "New yorkers can complain by visiting NYC's online customer service, text messages, phone calls, skype, etc.NYC 311 dataset covers all aspects of citizen's life in New York, which can be roughly divided into the following categories: Benefit& Support, Business& Consumers, Courts& Law, Culture& Recreation, Education, Employment, Environment, Garbage& Recycling, Government& Elections, Health, Housing& Buildings, Noise, Pets,Pests& Wildlife, Public safety, Records, Sidewalks,Streets& highways, Taxes, Transportation.\n",
    "<br>\n",
    "\n",
    "NYC311's mission is to provide the public with fast, convenient city government services and information, while providing the best customer service. It also helps organizations improve the services they offer, allowing them to focus on their core tasks and manage their workloads effectively. Meanwhile, NYC 311also provides insights into improving city government through accurate and consistent measurement and analysis of service delivery.\n",
    "\n",
    "<br>\n",
    "Moreover,NYC311 is available 24 hours a day, 7 days a week, 365 days a year. \n",
    "Not only does NYC311 offer an online translation service in more than 50 languages, but users can call the 311 hotline in more than 175 languages if their language is not included.In addition, people who are deaf, hard of hearing or have language impairment can also complaint with special help such as video relay service (VRS).\n",
    "\n",
    "<br>We believe there is a lot of information to explore in such a large and data-rich dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why did you choose this particular dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it was impossible for us to conduct a comprehensive analysis of this incredible hugh dataset, so after preliminary statistics, we chose the category with the most cumulative complaints over the past decade: Noise.\n",
    "\n",
    "<br>First of all, when it comes to environmental pollution, people may first think of air, soil, water and other aspects, but noise pollution, as an invisible and intangible existence, has the same impact on us that cannot be ignored.As a serious \"urban disease\", noise pollution has increasingly become the focus of modern urban life. New York, as a prosperous international city, also has such problems.\n",
    "\n",
    "<br>Moreover, We want to study the noise complaints in New York and analyze them from both spatial perspective and temporal perspective. We hope to learn something about the urban conditions, economic development, residents' living conditions and traffic conditions, etc, in the five boroughs of New York through the noise complaints. Moreover, we wonder whether noise complaints can be used to describe the overall development and brief condition of New York City over a 10-year period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we want to share interesting insights to the readers from noise analysis. The seemingly boring government complaints hotline actually contains many interesting insights, which not only reflect the people's life in New York, but also provide some directions and suggestions for the government to improve the city service. \n",
    "Also, via the analysis of noise complaints in NYC, we hope users could understand the characters, living habits, preferences and cultural backgrounds of the residents in the five different boroughs of New York.\n",
    "<br>\n",
    "\n",
    "Further more, we hope that readers can freely access the information they find useful through interactive map and interactive bar by reading the New York stories presented by us, which can not only increase readers' understanding but also make reading more participatory and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File 311-2019-all.csv does not exist: '311-2019-all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec60ec4a1c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_origin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'311-2019-all.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'311-All-Concise-with-IncidentZip.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File 311-2019-all.csv does not exist: '311-2019-all.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_origin=pd.read_csv('311-2019-all.csv')\n",
    "df=pd.read_csv('311-All-Concise-with-IncidentZip.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 22.8M rows and 41 columns with size of 12GB. The dataset is shown as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_origin.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes are shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_origin.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a bar chart to show the 15 most frequent complaint type in New York during 2010~2020 to get some inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "complaint_count=df['Complaint Type'].value_counts()\n",
    "complaint_count.iloc[0:20]\n",
    "\n",
    "title='The 15 most frequent complaint type in New York during 2010~2020'\n",
    "to_display=complaint_count[0:15]\n",
    "f,p=plt.subplots(figsize=(20,15))\n",
    "p.bar(to_display.index,to_display.values)\n",
    "p.tick_params(axis='x',labelrotation=90)\n",
    "p.tick_params(labelsize=15)\n",
    "p.set_title(title,fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure, we found noise is the most reported complain type, which inspired us to discover more about it. For temporal and spatial analysis of Noise, we think only 9 attributes are relevant and retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These attributes are used for the different purpuses.\n",
    "* Created Date\\Closed Date: Used for label the time of each cases, serve for temporal analysis. It is stored in String.\n",
    "* Complaint Type: Main complaint types.It has 439 different values and provide a fundationtal classification of each complaint type.\n",
    "* Descriptor: For some main types, people may be confused for the names are ambiguous. This is associated to the Complaint Type, and provides further detail on the incident or condition. Descriptor can be seen as a set of sub-type of each Complaint Type. It has 1168 different values.\n",
    "* Location Type: Describes the type of location used in the address information. It corresponds to 'Complaint Type' as well as 'Descriptor' so that it can provide more explaination. For example, The location type, Store, corresponds to the complaint type of Noise - Commercial. It helps when the Complaint Type and Descriptor are ambiguous.\n",
    "* Incident Zip: Incident location zip code. It describes the zipcode of the block where the incident took place. It contains some irrelevent information and NaN values and the method to handle with is explained in 2.2\n",
    "* Borough: Name of the borough where the incident took place. It contains some irrelevent information and NaN values and the method to handle with is explained in 2.2\n",
    "* Latitude/Longitude: Coordinates of the incident position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data preprocessing and cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, We adopt Created Data as the time when the incident happened. It has to be transformed to pandas datetime objets so that we can extract the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suitform='%m/%d/%Y %H:%M:%S %p'\n",
    "df['TransCDatetime']=pd.to_datetime(df['Created Date'],format=suitform)\n",
    "df['month']=[i.month+(i.year-2010)*12 for i in df['TransCDatetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nan=df['TransCDatetime'].isna()\n",
    "time_nan.sum()\n",
    "print('The percentage of nan value of for created time is {:10.2f}%'.format(time_nan.sum()/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successffully transformed the format of datatime, which indicates all the elements are valid and also no NaN value is detected in the attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complaint type and Descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For noise analysis, we will have the five following main types. We only focus on the noise types that are in the 50 top complaints type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_count=df['Complaint Type'].value_counts()\n",
    "TOP_COMPLAINTS=50\n",
    "cared=complaint_count.iloc[0:TOP_COMPLAINTS].index\n",
    "Noise_type=[]\n",
    "for i in cared:\n",
    "    if 'oise' in i:\n",
    "        Noise_type.append(i)\n",
    "Noise_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each main type, we also have subtypes which are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Noise_summary=dict()\n",
    "for i in Noise_type:\n",
    "    temp=df[df['Complaint Type']==i]\n",
    "    Noise_summary[i]=temp\n",
    "\n",
    "for i in Noise_type:\n",
    "    print('The main type is', i)\n",
    "    subtype=Noise_summary[i]['Descriptor'].unique()\n",
    "    for j in subtype:\n",
    "        print('    The subtype is',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have 5 maintypes and 36 subtypes, which are considered all main types and subtypes are valid, so that no further cleaning and processing are demanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Incident Zip and Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created Choropleth map for distribution of noise cases acrss different blocks in 2019, by counting the number of cases for each zipcode. \n",
    "\n",
    "In the first place, the data quality for the ten years (2010~2020) is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Incident Zip'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main problems for the attribute Zipcode have been detected:\n",
    "* NaN values\n",
    "* Zipcode with invalid characters,e.g. alphabet\n",
    "\n",
    "It is necessary to figure out the the percentage of the valid values. It is calculated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify each item if they have the following problems: nan, invalid character\n",
    "import re\n",
    "zipnan=df['Incident Zip'].isna()\n",
    "zipnan=zipnan.to_numpy()\n",
    "zipalph=[]\n",
    "for i in df['Incident Zip']:\n",
    "    a=(re.search('[a-zA-Z]', str(i))!=None)\n",
    "    b=(re.search('[-]', str(i))!=None)\n",
    "    zipalph.append(a and b)\n",
    "zipalph=np.array(zipalph)\n",
    "percentage=zipalph.sum()+zipnan.sum()\n",
    "print('The percentage of invalid value of the whole dataset is {:10.2f}%'.format(percentage/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of invalid values is 5.79%, which is acceptable because we mainly focus on the overall distribution and trend of some focused features.\n",
    "\n",
    "However, in the interactive map, we presented the noise distribution in 2019 so that a particular attention should be paid to the data quality for this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year']=[i.year for i in df['TransCDatetime']]\n",
    "df_2019=df[df['year']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "zipnan1=df_2019['Incident Zip'].isna()\n",
    "zipnan1=zipnan1.to_numpy()\n",
    "zipalph1=[]\n",
    "for i in df_2019['Incident Zip']:\n",
    "    a=(re.search('[a-zA-Z]', str(i))!=None)\n",
    "    b=(re.search('[-]', str(i))!=None)\n",
    "    zipalph1.append(a and b)\n",
    "zipalph1=np.array(zipalph)\n",
    "percentage=zipalph1.sum()+zipnan1.sum()\n",
    "print('The percentage of invalid value for 2019 is {:10.2f}%'.format(percentage/df_2019.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that it is of better quality compared to the dataset(3.16% of 2019 to 5.79% to 2010~2020), which indicates improvement in data collection by the government. \n",
    "\n",
    "But we still want to do correction to the invalid values for 2019. K-nearest-neighbours(KNN) is the machine learning algorithm can be adopted for this problem because the zipcode is determined by coordinates of the point. Therefore, the first thing came to our mind is that the probability of invalid coordinate given invalid zipcode because zipcode should be predicted based on coordinates.\n",
    "\n",
    "Here, outliers in coordinates are detected with boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=df_2019['Latitude'].isna() & df_2019['Longitude'].isna()\n",
    "b=df_2019['Latitude'].isna()\n",
    "print('Total number of NaN in Latitude is {}'.format(a.sum()))\n",
    "print('Total number of NaN in Latitude or Longitude is {}'.format(b.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two numbers are equal, which means that if NaN is present in Latitude, it is also NaN in the correspoding longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,p=plt.subplots(1,2,sharex=True,figsize=(20,5))\n",
    "font=18\n",
    "#titledict={'x':0.02,'y':0.9}\n",
    "p[0].set_title('Latitude of noise cases',fontsize=font)\n",
    "p[0].boxplot(df_2019[~b]['Latitude'])\n",
    "p[0].tick_params(labelsize=font)\n",
    "p[1].set_title('Longitude of noise cases',fontsize=font)\n",
    "p[1].boxplot(df_2019[~b]['Longitude'])\n",
    "p[1].tick_params(labelsize=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the NaN values, all the cocordinates are in the right range. We considered no other outliers included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latnan1=b\n",
    "latnan1=latnan1.to_numpy()\n",
    "print('The percentage of invalid value of coordinates for 2019 is {:10.2f}%'.format(latnan1.sum()/df_2019.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of invalid values is 5.31%. And then we are going to calculate the probability of invalid coordinate given invalid zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notused=0\n",
    "for i in range(df_2019['Incident Zip'].shape[0]):\n",
    "    if latnan1[i] and zipnan1[i] and ~zipalph1[i]:\n",
    "        notused+=1\n",
    "print('The percentage of invalid coordinate given invalid zipcode{:10.2f}%'.format(notused/percentage*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that for the invalid zip code, it is 99.83% likely not having its coordinates. Therefore KNN will not be effective and it is also inferred that if the government did not record the zipcode, they also did not get the position of the case. \n",
    "However, in the interactive map, we presented the noise distribution in 2019 so that a particular attention should be paid to the data quality for this year.\n",
    "\n",
    "Based on above analsis, we discarded the invalid values for zipcode and it will not have great effect on the analysis result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a intearactive bar chart displaying distributions of various noise types in different boroughs.\n",
    "\n",
    "In the first place, the data quality for the ten years (2010~2020) is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Borough'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is shown that the invalid value is 'Unspecified', for which we have calculated its percentage in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_whole=(df['Borough']=='Unspecified')\n",
    "print('The percentage of invalid value of the whole dataset is {:10.2f}%'.format(unspecified_whole.sum()/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of invalid values is 5.35%, which is acceptable to discard the invalid values because we mainly focus on the overall distribution and trend of some focused features.\n",
    "However, in the interactive bar chart, we presented distributions of various noise types in different boroughs in 2019 so that a particular attention should be paid to the data quality for this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_2019=(df_2019['Borough']=='Unspecified')\n",
    "print('The percentage of invalid value of the whole dataset is {:10.2f}%'.format(unspecified_2019.sum()/df_2019.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that it is of better quality compared to the dataset(0.91% of 2019 to 5.35% to 2010~2020), which indicates improvement in data collection by the government.\n",
    "As for our analysis, We discarded the unspeicifed value and it will not have a great influence on our analysis result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary of the dataset after cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset covers a great number of complaint types, it is necessary to narrow it down to the main ones to obtain the main trends and features of noise in the New York city. After data cleanning and preprocessing, the dataset only contains the necessary attributes for the report. The datasize has 22662415 rows and 10 colomns (of original attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The proportion of noise out of the whole cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in df['Complaint Type']:\n",
    "    if 'oise' in i:\n",
    "        count+=1\n",
    "print('The percentage of noise out of the whole dataset is {:10.2f}%'.format(count/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sum up main types and sub types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_noise=df[df['Complaint Type'].str.contains('oise', regex=False)]\n",
    "counts=main_noise['Complaint Type'].value_counts()\n",
    "counts=counts.iloc[0:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "counts.plot(kind='bar')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('The sum of each main type (The 5 most frequently)',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequently main type is Noise - Residiential, which shows that the noise cases rae mostly reported by the residents. Below, we also plot the 15 most frequently subtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_noise=main_noise['Descriptor'].value_counts()\n",
    "plt.figure(figsize=(12,8))\n",
    "sub_noise.plot(kind='bar')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('The sum of each subtype (The 15 most frequently)',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The proportion of the considred noise cases out of the whole noise cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sum()/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the monthly trend of main types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,p=plt.subplots(len(Noise_type),figsize=(60,200))\n",
    "m=0\n",
    "month_range=np.arange(df['month'].min(),df['month'].max()+1)\n",
    "month_range_scarce=np.arange(df['month'].min(),df['month'].max()+1,5)\n",
    "for i in Noise_type:\n",
    "    monthly=pd.Series(np.zeros(len(month_range)+1),dtype='int32')\n",
    "    drawn=df[df['Complaint Type']==i]['month'].value_counts()\n",
    "    print('I am doing ', i)\n",
    "    for j in drawn.index:\n",
    "        monthly.loc[j]=drawn[j]\n",
    "    p[m].bar(month_range,monthly[month_range])\n",
    "    p[m].set_title(i,size=60)\n",
    "    p[m].tick_params(axis='x',labelrotation=90)\n",
    "    p[m].set_ylim(0,1.2*monthly.max(axis=0))\n",
    "    p[m].tick_params(labelsize=30)\n",
    "    p[m].set_xticks(month_range)\n",
    "    m+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that for the five main crime types, they all show an increasing trend from 2010 to 2020 and seasonal fluctuation.\n",
    "\n",
    "We can obtain more information if the monthly trend of each subtype is plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the monthly trend of sub types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in Noise_type:\n",
    "#     m=0\n",
    "#     subtype=Noise_summary[i]['Descriptor'].unique()\n",
    "#     print('Len of subtype',len(subtype))\n",
    "#     f,p=plt.subplots(len(subtype),figsize=(60,200))\n",
    "#     plt.subplots_adjust(hspace = 0.4)\n",
    "#     for j in subtype:\n",
    "#         monthly=pd.Series(np.zeros(len(month_range)+1),dtype='int32')\n",
    "#         drawn=Noise_summary[i][Noise_summary[i]['Descriptor']==j]['month'].value_counts()\n",
    "#         print('I am doing ',i,j)\n",
    "#         for k in drawn.index:\n",
    "#             monthly.loc[k]=drawn[k]\n",
    "# #        print(monthly[month_range])\n",
    "#         p[m].bar(month_range,monthly[month_range])\n",
    "#         p[m].set_title(i+':  '+j,size=60)\n",
    "#         p[m].tick_params(axis='x',labelrotation=90)\n",
    "#         p[m].set_ylim(0,1.2*monthly.max(axis=0))\n",
    "#         p[m].tick_params(labelsize=30)\n",
    "#         p[m].set_xticks(month_range_scarce)\n",
    "#         m+=1\n",
    "\n",
    "m=0\n",
    "n=0   \n",
    "f,p=plt.subplots(18,2,figsize=(60,100))\n",
    "for i in Noise_type:  \n",
    "    subtype=Noise_summary[i]['Descriptor'].unique()\n",
    "#    print('Len of subtype',len(subtype))\n",
    "#     if len(subtype)%2==1:\n",
    "#         rows=len(subtype)//2+1\n",
    "#     else:\n",
    "#         rows=len(subtype)//2\n",
    "    \n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    for j in subtype:\n",
    "        monthly=pd.Series(np.zeros(len(month_range)+1),dtype='int32')\n",
    "        drawn=Noise_summary[i][Noise_summary[i]['Descriptor']==j]['month'].value_counts()\n",
    "#        print('I am doing ',i,j)\n",
    "        for k in drawn.index:\n",
    "            monthly.loc[k]=drawn[k]\n",
    "#        print(monthly[month_range])\n",
    "#        print(m,n)\n",
    "        p[m][n].bar(month_range,monthly[month_range])\n",
    "        p[m][n].set_title(i+':  '+j,size=30)\n",
    "        p[m][n].tick_params(axis='x',labelrotation=90)\n",
    "        p[m][n].set_ylim(0,1.2*monthly.max(axis=0))\n",
    "        p[m][n].tick_params(labelsize=30)\n",
    "        p[m][n].set_xticks(month_range_scarce)\n",
    "        n+=1\n",
    "        if n==2:\n",
    "            m+=1\n",
    "            n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial analysis, we focuses only on the subtype of noise with complete data (all available from 2010 to 2020). Generally they show the seasonal pattern of more cases in the summer while less in the winter. Besides that, we sorted them subtypes into three catogories in terms of overall trend.\n",
    "* Ascending trendï¼šmost of the subtypes are in ascending trend, mostly relevant to human activity. e.g. Loud Music/Party, Loud Talking.\n",
    "* Stable: only a few, mostly irrelevant to human activities, e.g. Barking Dog.\n",
    "* Dscending trend: only one, Jack Hammering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of coordinates distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "main_noise=main_noise[~np.isnan(main_noise['Latitude'])]\n",
    "font=18\n",
    "# histogram\n",
    "f,p=plt.subplots(2,1,figsize=(10,8))\n",
    "f.tight_layout(pad=3.0)\n",
    "p[0].hist(main_noise['Latitude'],bins=50,alpha=0.75,edgecolor = 'white', linewidth = 1.2)\n",
    "p[0].tick_params(labelsize=font)\n",
    "p[0].set_title('Histogram and KDE of Latitude',fontsize=font)\n",
    "# KDE\n",
    "density = gaussian_kde(main_noise['Latitude'])\n",
    "m,n=np.histogram(main_noise['Latitude'],bins=50)\n",
    "p[1].plot(n,density(n))\n",
    "p[1].tick_params(labelsize=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,p=plt.subplots(2,1,figsize=(10,8))\n",
    "f.tight_layout(pad=3.0)\n",
    "p[0].hist(main_noise['Longitude'],bins=50,alpha=0.75,edgecolor = 'white', linewidth = 1.2)\n",
    "p[0].tick_params(labelsize=font)\n",
    "p[0].set_title('Histogram and KDE of Longitude',fontsize=font)\n",
    "# KDE\n",
    "density = gaussian_kde(main_noise['Longitude'])\n",
    "m,n=np.histogram(main_noise['Longitude'],bins=50)\n",
    "p[1].plot(n,density(n))\n",
    "p[1].tick_params(labelsize=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the histogram, we observed how the coordinates are distributed and it fits the territorial shape of New York city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Relevant talk about your machine leanrning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, the focus is about statistical analysis, visualization and story-telling. No machine learning problems are involved in the analysis, except the case that we planned to use K-nearest-neighbours to make some correction for the default or invalid values in the attribute 'Incident Zip'. As it is described in the data cleaning section, it is impossible to implement KNN for mostly both coordinates and zipcode are missing at the same time while other attributes are considered irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal & Heer). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visual narrative, we chose the interactive slideshow, which we thought would be a good way to balance author-driven and reader-driven stories. There is an overall time narrative structure (e.g., slideshow), however, at some point, the user can manipulate the interaction visualization(interactive map and interactive bar in this project) to see more detailed information so that the reader can better understand the pattern or extract more relevant information (e.g., via interacting with a slideslideshow). Readers should also be able to control the reading progression themselves.For highlighting, zooming is conducted by us, readers can further explore the details that arouse their interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal & Heer) Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear ordering is selected by us in order to form a complete story line, hover details and selection are conducted in interactive parts. We maintain these can increase the reader's sense of participation and interactivity in reading. In the messaging section, headlines, annotations,introductry and summary are used. The headline give the readers the guidance about the specific content of the article while the annotation help readers get more information description.The introduction plays the role of arousing readers' interest and attracting them to further reading, while the summary conclude the content and stimulate readers' thinking, both of which give readers have a complete concept of the whole story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualizaition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the visualizations you've chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interactive choropleth map for distribution of noise cases acrss different blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an interactive choropleth map which shows not only overall distribution of the reported cases but also detailed information of each block. \n",
    "\n",
    "The color of one block indicates how many repored noise cases per hectare in it and readers can easily get a good understanding of the overall distribution with reference to the color bar. \n",
    "\n",
    "Besides, when you put your mouse on a maker and click it, you will get the zip number, block name and the number of cases per hectare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distributions of various noise types in different boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an interactive bar that shows the distribution of top ten noise subtypes in the five boroughs of New York."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sorted out the top 10 sub-noise types in terms of frequency and calculatd the percentage for each borough. The x axis presents the 10 noise type while the y axis illustrates the percentage for each borough. When the mouse is moved onto the bar, it shows the accruate value of percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From interactive choropleth map and bar chart, readers can get a general understanding of the problem but also the detailed information as their interest. Also, we provide our own story line to tell the readers what we have found and want them to know and use necessary supplementary material (Image) to help readers better understand. These storyline origniates from the phenomenon presented in the interactive visualization. Therefore, we think they are the right tools for the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The outliers and invalid values in the dataset but they consistute quite a small proportion(less than 5%) of the data we are concered. \n",
    "* All the codes work well and the result fits our genenral understanding of the problem but also the relevant information we obtained from the Internet. \n",
    "* We also find the right visualization tool to present our ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could be improved? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The interactive choropleth map is divided by blocks of different zipcode. We have observed that the size varies a lot across the blocks. All the data were sorted into some large block, which has resulted in the weakness that people cannot observe the distribution in the large block. We noticed that when we zoom in Manhattan and found some small blocks with high density and then realized that the uneven distribution in the large block was ignored. Heat map can be used to solve this problem but it cannot provide detailed information that we wanted to present to the readers. We consider the interactive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our analysis was conducted by finding the information we thought related to the phenomenon. It has explained something but in some cases we are not able to know if it is the cause. We believe more exploration into some problems are worthy and more information and advanced mathematical tools are demanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There may be other interesting aspect of data that deserves to be explored. Heat water problem is the second most frequently reported category, which may also contain some interesting insight. Also, the relationship between differen noise types is also worthy to explore. But we think it is not very relevant to the storyline in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
